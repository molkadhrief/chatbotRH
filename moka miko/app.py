import json
import numpy as np
import faiss
import torch
import os
from langdetect import detect
from transformers import AutoTokenizer, AutoModel
from groq import Groq

class ChatbotRH:
    def __init__(self, json_path, groq_api_key):
        """Initialise le systÃ¨me RAG avec FAISS et le modÃ¨le Groq."""
        self.data_store = []
        self.index = faiss.IndexFlatL2(384)  # 384 dimensions pour MiniLM

        # ModÃ¨le de vectorisation de texte
        self.text_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        self.text_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

        # Client Groq
        self.groq_client = Groq(api_key=groq_api_key)

        # Charger et indexer les donnÃ©es JSON
        self.load_json_data(json_path)

    def load_json_data(self, json_path):
        """Charge les donnÃ©es JSON et les indexe dans FAISS"""
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        for entry in data:
            # CrÃ©er un texte combinÃ© pour la recherche
            combined_text = f"{entry.get('question', '')} {entry.get('answer', '')} {' '.join(entry.get('paraphrases', []))}"
            
            if not combined_text.strip():
                continue  # Ignorer les entrÃ©es vides
            
            text_embedding = self._vectorize_text(combined_text)

            # Stocker les donnÃ©es
            self.data_store.append({
                "category": entry.get("category", ""),
                "subcategory": entry.get("subcategory", ""),
                "question": entry.get("question", ""),
                "answer": entry.get("answer", ""),
                "paraphrases": entry.get("paraphrases", []),
                "combined_text": combined_text
            })

            # Ajouter l'embedding Ã  FAISS
            self.index.add(np.array([text_embedding]))

    def _vectorize_text(self, text):
        """Convertit un texte en vecteur"""
        inputs = self.text_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.text_model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy().flatten()

    def detect_language(self, text):
        """DÃ©tecte la langue de la question (franÃ§ais ou anglais)"""
        try:
            lang = detect(text)
            return "fr" if lang == "fr" else "en"
        except:
            return "fr"  # Par dÃ©faut, rÃ©pondre en franÃ§ais pour les RH

    def query(self, question, k=3):
        """Recherche dans FAISS et gÃ©nÃ¨re une rÃ©ponse avec Groq"""
        query_embedding = self._vectorize_text(question)

        # Recherche des k meilleurs rÃ©sultats
        distances, indices = self.index.search(np.array([query_embedding]), k)

        # VÃ©rifier si des rÃ©sultats sont retournÃ©s
        valid_indices = [i for i in indices[0] if i >= 0 and i < len(self.data_store)]
        if not valid_indices:
            return "DÃ©solÃ©, je n'ai pas trouvÃ© d'information pertinente dans ma base de connaissances RH. Pouvez-vous reformuler votre question ?"

        # Construire un contexte dÃ©taillÃ©
        context_entries = []
        for i in valid_indices:
            entry = self.data_store[i]
            context_entries.append(f"""
CatÃ©gorie: {entry['category']} - {entry['subcategory']}
Question: {entry['question']}
RÃ©ponse: {entry['answer']}
""")
        
        context = "\n".join(context_entries)

        # DÃ©tection de la langue de la question
        lang = self.detect_language(question)

        # CrÃ©er le prompt adaptÃ© Ã  la langue
        if lang == "fr":
            system_prompt = """Vous Ãªtes un assistant RH expert et bienveillant. Votre rÃ´le est d'aider les employÃ©s avec leurs questions relatives aux ressources humaines.

Instructions:
- RÃ©pondez de maniÃ¨re claire, prÃ©cise et professionnelle
- Utilisez les informations du contexte fourni pour donner des rÃ©ponses exactes
- Si l'information n'est pas dans le contexte, dites-le clairement
- Soyez empathique et serviable
- RÃ©pondez en franÃ§ais

Contexte RH disponible:
{context}

Question de l'employÃ©: {question}

RÃ©ponse:"""
        else:
            system_prompt = """You are an expert and helpful HR assistant. Your role is to help employees with their human resources questions.

Instructions:
- Answer clearly, precisely and professionally
- Use the information from the provided context to give accurate answers
- If the information is not in the context, state it clearly
- Be empathetic and helpful
- Respond in English

Available HR context:
{context}

Employee question: {question}

Answer:"""

        try:
            # Appel Ã  l'API Groq
            chat_completion = self.groq_client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": "Vous Ãªtes un assistant RH expert et bienveillant." if lang == "fr" else "You are an expert and helpful HR assistant."
                    },
                    {
                        "role": "user",
                        "content": system_prompt.format(context=context, question=question)
                    }
                ],
                model="llama3-8b-8192",  # ou "mixtral-8x7b-32768" selon vos prÃ©fÃ©rences
                temperature=0.3,
                max_tokens=1024,
            )
            
            response = chat_completion.choices[0].message.content

            # Ajouter les sources utilisÃ©es
            sources_info = "\n\nğŸ“š **Sources consultÃ©es:**\n"
            for i, idx in enumerate(valid_indices):
                entry = self.data_store[idx]
                sources_info += f"â€¢ {entry['category']} - {entry['subcategory']}\n"
            
            response += sources_info
            
            return response

        except Exception as e:
            return f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse: {str(e)}"

    def chat_interactive(self):
        """Mode chat interactif"""
        print("ğŸ¤– Chatbot RH - Tapez 'quit' pour quitter")
        print("=" * 50)
        
        while True:
            question = input("\nğŸ‘¤ Votre question RH: ").strip()
            
            if question.lower() in ['quit', 'exit', 'quitter']:
                print("ğŸ‘‹ Au revoir ! N'hÃ©sitez pas Ã  revenir si vous avez d'autres questions RH.")
                break
            
            if not question:
                continue
                
            print("\nğŸ¤– RÃ©ponse:")
            response = self.query(question)
            print(response)
            print("-" * 50)


# === Exemple d'utilisation ===
if __name__ == "__main__":
   
    
    # Initialiser le chatbot
    chatbot = ChatbotRH("data.json", GROQ_API_KEY)
    
    # Test avec quelques questions
    print("ğŸ§ª Tests du chatbot RH:")
    print("=" * 50)
    
    questions_test = [
        "Comment rÃ©cupÃ©rer ma fiche de paie ?",
        "Quand est-ce que je suis payÃ© ?",
        "Comment poser des congÃ©s ?",
        "Quel est le remboursement pour les lunettes ?",
        "How can I check my leave balance?"
    ]
    
    for question in questions_test:
        print(f"\nâ“ Question: {question}")
        response = chatbot.query(question)
        print(f"ğŸ¤– RÃ©ponse: {response}")
        print("-" * 50)
    
    # Lancer le mode interactif (dÃ©commentez si souhaitÃ©)
    # chatbot.chat_interactive()
    
    